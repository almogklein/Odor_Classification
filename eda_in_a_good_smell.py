# -*- coding: utf-8 -*-
"""EDA_in_a_good_smell.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15nGCPtmUrRzsVgx-kZebEfgfFtPeXHj8
"""

!pip install -U https://github.com/pandas-profiling/pandas-profiling/archive/master.zip
!pip install -U pandas-profiling

profile = ProfileReport(df_uni2, title="Profiling Report")
profile
profile.to_file("EDA_in_a_good_smell.html")

from google.colab import drive
drive.mount("/content/gdrive")



# Commented out IPython magic to ensure Python compatibility.
import os
import pathlib


# %matplotlib notebook
# %matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
import plotly.io as plt_io
import plotly.graph_objects as go
# %matplotlib inline
import seaborn as sns
import plotly.express as px
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
#from pandas_profiling import ProfileReport


# Machine Learning
from sklearn import metrics
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA,  IncrementalPCA
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

from scipy.stats import mannwhitneyu

def creat_labaled_32(path): 
    
  df_all_3 = pd.read_csv(path, encoding='ISO-8859-1', low_memory=False)
  df_all_3.drop(['Unnamed: 1', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Flag', 'Time'], axis = 1, inplace = True) 

  df_all_3['Batch Index'] = 0
  df_all_3['Batch Sample Index'] = 0
  
  counter = 0
  flip_count = 0
  sample_index_count = 0

  def base_categorise(row):
    a = [4, 6, 10, 11]
    b = [21, 231, 232, 26, 32]
    if row['Batch Number'] in a:
      return 0
    elif  row['Batch Number'] in b:
      return 1
  def batch_contegees(row):
    b = row['Base']
    c = row['Contegees Number']
    bi = row['Batch Index'] + 1

    s = str(int(b))+'-'+str(int(c))+'-'+str(int(bi))
          
    return s

  df_all_3['Base'] = df_all_3.apply(lambda x: base_categorise(x), axis=1)

  for i in range(1, len(df_all_3['S1'])):

    if df_all_3.iloc[i, 1] == 1.0 and df_all_3.iloc[i-1, 1] == 1.0 and flip_count == 0:
      df_all_3.iloc[i, 33] = counter % 5
      sample_index_count+=1
      df_all_3.iloc[i, 34] = sample_index_count

    elif df_all_3.iloc[i, 1] != 1.0 and df_all_3.iloc[i-1, 1] == 1.0 and flip_count == 0:
      flip_count += 1
      sample_index_count = 0 
      df_all_3.iloc[i, 33] = counter % 5
      df_all_3.iloc[i, 34] = sample_index_count

    elif df_all_3.iloc[i, 1] != 1.0 and df_all_3.iloc[i-1, 1] != 1.0 and flip_count == 1:
      df_all_3.iloc[i, 33] = counter % 5
      sample_index_count+=1
      df_all_3.iloc[i, 34] = sample_index_count


    elif df_all_3.iloc[i, 1] == 1.0 and df_all_3.iloc[i-1, 1] != 1.0 and flip_count == 1:
      flip_count -= 1
      counter += 1
      sample_index_count = 0 
      df_all_3.iloc[i, 33] = counter % 5
      df_all_3.iloc[i, 34] = sample_index_count

  df_all_3['Contegees Number'] = df_all_3['Batch Number'] 
  df_all_3.drop(['Batch Number'], axis = 1, inplace = True) 
  df_all_3.dropna()
  df_all_3['Batch CC'] = df_all_3.apply(lambda x: batch_contegees(x), axis=1)
  
#     df_all_3.to_csv(r'C:\Users\almog\test_____________.csv')
  
  return df_all_3

def creat_labaled_8(path): 
    
  df_all_3 = pd.read_csv(path, encoding='ISO-8859-1', low_memory=False)
  df_all_3['Base'] = 1
  df_all_3['Batch Sample Index'] = df_all_3['Time']
  
  counter = 0
  flip_count = 0
  sample_index_count = 0

  def Batch_CC(row): return row['CC'].split(' ')[2].split('.')[0][-1]
  
  def Contegees_Number(row): return row['CC'].split(' ')[1]
  
  def batch_index(row):
    b = row['Base']
    c = row['CC'].split(' ')[1]
    bi = row['CC'].split(' ')[2].split('.')[0][-1]

    s = str(int(b))+'-'+str(int(c))+'-'+str(int(bi))
    return s
  
  df_all_3['Batch Index'] = df_all_3.apply(lambda x: Batch_CC(x), axis=1)
  df_all_3['Batch CC'] = df_all_3.apply(lambda x: batch_index(x), axis=1)
  df_all_3['Contegees Number'] = df_all_3.apply(lambda x: Contegees_Number(x), axis=1)
  
  
  
  df_all_3.drop(['CC', 'Time'], axis = 1, inplace = True) 

  
  return df_all_3

def filter_outliers(df, threshold_dict):
  for column, threshold in threshold_dict.items():
    mean = df[column].mean()
    std = df[column].std()
    lower_bound = mean - threshold * std
    upper_bound = mean + threshold * std
    df = df[(df[column] > lower_bound) & (df[column] < upper_bound)]
  return df

path_32 = '/content/gdrive/MyDrive/grate_smell/all 3 tests.csv' #r'C:\Users\almog\all 3 tests.csv'
df_32 = creat_labaled_32(path_32)

path_8 = '/content/gdrive/MyDrive/grate_smell/PEN3.csv' #r'C:\Users\almog\PEN3.csv'
df_8 = creat_labaled_8(path_8)



df_32.sample(10)

df_8.sample(10)



print(pd.value_counts(df_32.dtypes))
print('\n\n')  
df_32.info()

df_32.dropna(inplace=True, axis=1)
plt.figure(figsize = (10,8))
plt.imshow(df_32.isna(), aspect = "auto", interpolation = "nearest", cmap="gray")
plt.xlabel("Column Number")
plt.ylabel("Sample Number")

df_32.describe()

unique_values = df_32.select_dtypes(include="number").nunique().sort_values()

unique_values.plot.bar(logy=True, figsize=(15,4), title="Unique values per feature")

print(pd.value_counts(df_8.dtypes))
print('\n\n')  
df_8.info()

df_8.dropna(inplace=True, axis=1)
plt.figure(figsize = (10,8))
plt.imshow(df_8.isna(), aspect = "auto", interpolation = "nearest", cmap="gray")
plt.xlabel("Column Number")
plt.ylabel("Sample Number")

df_8.describe()

unique_values = df_8.select_dtypes(include="number").nunique().sort_values()

unique_values.plot.bar(logy=True, figsize=(15,4), title="Unique values per feature")

df_32.plot(lw = 0, marker = ".", subplots = True, layout = (-1,4), figsize = (20,20), markersize = 1)

s = 3
threshold_dict = {
  "S17": s, "S18": s, "S19": s, "S20": s, "S21": s, "S22": s,
    "S23": s, "S24": s, "S25": s, "S26": s, "S27": s, "S28": s, 
    "S29": s, "S30": s, "S31": s, "S32": s
}

filtered_df_32 = filter_outliers(df_32, threshold_dict)
filtered_df_32 = filtered_df_32[~(filtered_df_32['S1'] == 1)]

filtered_df_32['S18'].plot(figsize = (20,10))

filtered_df_32.plot(lw = 0, marker = ".", subplots = True, layout = (-1,4), figsize = (20,20), markersize = 1)

filtered_df_32.hist(bins = 10, figsize = (20,20), layout=(-1,4), edgecolor = "black")
plt.tight_layout();

df_8.plot(lw = 0, marker = ".", subplots = True, layout = (-1,4), figsize = (20,20), markersize = 1)

s = 5
threshold_dict = {
  "S1": s, "S2": s, "S3": s, "S4": s, "S5": s, "S6": s, "S7": s, "S8": s, 
    "S9": s, "S10": s
}

filtered_df_8 = filter_outliers(df_8, threshold_dict)
# filtered_df_32 = filtered_df_32[~(filtered_df_32['S1'] == 1)]

filtered_df_8['S1'].plot(figsize = (20,10))

filtered_df_8.plot(lw = 0, marker = ".", subplots = True, layout = (-1,4), figsize = (20,20), markersize = 1)

filtered_df_8.hist(bins = 10, figsize = (20,20), layout=(-1,4), edgecolor = "black")
plt.tight_layout();







most_frequent_entery = filtered_df_32.mode().iloc[0]

most_frequent_entery
df_freq = filtered_df_32.eq(most_frequent_entery.values , axis = 1)
df_freq = df_freq.mean().sort_values(ascending = False)


df_freq.plot.bar(figsize = (20, 5))
display(df_freq)

unique_values = filtered_df_32.select_dtypes(include="number").nunique().sort_values()

unique_values.plot.bar(logy=True, figsize=(15,4), title="Unique values per feature")

df_corr = filtered_df_32.corr(method = "pearson")

labels = np.where(np.abs(df_corr) > 0.99 , "X", np.where(np.abs(df_corr) > 0.8 , "S", np.where(np.abs(df_corr) > 0.5 , "M", np.where(np.abs(df_corr) > 0.25 , "W", " "))))

plt.figure(figsize = (20, 20))
sns.heatmap(df_corr, mask = np.eye(len(df_corr)), square=True , center = 0, annot=labels, fmt='', linewidths=0.5,
            cmap = "vlag", cbar_kws={"shrink":0.8})

most_frequent_entery = filtered_df_8.mode().iloc[0]

df_freq = filtered_df_8.eq(most_frequent_entery.values , axis = 1)
df_freq = df_freq.mean().sort_values(ascending = False)


df_freq.plot.bar(figsize = (20, 5))
display(df_freq)

unique_values = filtered_df_8.select_dtypes(include="number").nunique().sort_values()

unique_values.plot.bar(logy=True, figsize=(15,4), title="Unique values per feature")

df_corr = filtered_df_8.corr(method = "pearson")

labels = np.where(np.abs(df_corr) > 0.99 , "X", np.where(np.abs(df_corr) > 0.8 , "S", np.where(np.abs(df_corr) > 0.5 , "M", np.where(np.abs(df_corr) > 0.25 , "W", " "))))

plt.figure(figsize = (20, 20))
sns.heatmap(df_corr, mask = np.eye(len(df_corr)), square=True , center = 0, annot=labels, fmt='', linewidths=0.5,
            cmap = "vlag", cbar_kws={"shrink":0.8})

print(df_32.shape)
print(filtered_df_32.shape)

print(f'Data shape: ({filtered_df_32.shape[0]}, {filtered_df_32.shape[1]})\n')
print('Base value_counts:\n')
print(filtered_df_32['Base'].value_counts())

print('\n\nContegees  value_counts:\n')
print(filtered_df_32['Contegees Number'].value_counts())

print(df_8.shape)
print(filtered_df_8.shape)

print(f'Data shape: ({filtered_df_8.shape[0]}, {filtered_df_8.shape[1]})\n')
print('Base value_counts:\n')
print(filtered_df_8['Base'].value_counts())

print('\n\nContegees  value_counts:\n')
print(filtered_df_8['Contegees Number'].value_counts())

sns.pairplot(filtered_df_32, hue="Base", corner=True, diag_kind="kde", palette={0: "green", 1: "purple"})

sns.pairplot(filtered_df_8.iloc[:, 7:], hue="Contegees Number", corner=True, diag_kind="kde")

filtered_df_8

filtered_df_32.to_csv('/content/gdrive/MyDrive/grate_smell/32.csv')

filtered_df_8.to_csv('/content/gdrive/MyDrive/grate_smell/8.csv')

def process_dataframe(df, window_size, column_names):
  # Create a new dataframe with the same columns as the original
  new_df = pd.DataFrame(columns=column_names)
  
  # Iterate over the columns in the original dataframe
  for col in column_names:
    # Use a rolling window to calculate the mean of each window
    rolling_mean = df[col].rolling(window_size).mean()
    
    # Sort the rolling mean in descending order
    sorted_rolling_mean = rolling_mean.sort_values(ascending=False)
    
    # Take the top 10 values from the sorted rolling mean
    top_10 = sorted_rolling_mean.head(10)
    
    # Add the corresponding rows from the original dataframe to the new dataframe
    for index in top_10.index:
      new_df = new_df.append(df.loc[index,:], ignore_index=True)
  
  return new_df

window_size = 10
column_names = ["S1", "S2", "S3", "S4", "S5", "S6", "S7", "S8", "S9", "S10"] #, 'Base',	'Batch Sample Index',	'Batch Index',	'Batch CC',	'Contegees Number']
new_df = process_dataframe(filtered_df_8, window_size, column_names)

new_df['Contegees Number'].value_counts()

def average_rows(df, window_size, jump_size):
  # Create an empty DataFrame with the same columns as the input DataFrame
  new_df = pd.DataFrame(columns=df.columns)
  
  # Use the rolling method to calculate the mean of the window_size rows, with a jump_size between each window
  for i in range(0, df.shape[0]-window_size+1, jump_size):
    window = df.iloc[i: i+window_size, :]
    mean_row = window.mean()
    new_df = new_df.append(mean_row, ignore_index=True)
  return new_df

averaged_df = average_rows(filtered_df_8, window_size=10, jump_size=1)











df_32 = pd.read_csv('/content/gdrive/MyDrive/grate_smell/32_MAXMIN.csv')

df_8 = pd.read_csv('/content/gdrive/MyDrive/grate_smell/8_maxmin.csv')
# df_8.pop('Unnamed: 0')
df_8 = df_8.iloc[:12, :15]

df_8 = df_8.iloc[:12, :15]

df_32 = pd.read_csv('/content/gdrive/MyDrive/grate_smell/32.csv')
df_32.pop('Unnamed: 0')


df_8 = pd.read_csv('/content/gdrive/MyDrive/grate_smell/8.csv')
df_8.pop('Unnamed: 0')

filtered_df_32 = df_32.copy() 

filtered_df_8 = df_8.copy() 
filtered_df_8 = filtered_df_8.iloc[:, :-11]



df_mean_8 = filtered_df_8.copy().groupby(['Batch CC']).mean()
df_mean_8c = filtered_df_8.copy().groupby(['Contegees Number']).mean()

x_8 = filtered_df_8.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).copy().values
y_b_8 = filtered_df_8['Base'].copy().values
y_c_8 = filtered_df_8['Contegees Number'].copy().values
y_bi_8 = filtered_df_8['Batch Index'].copy().values
y_bcc_8 = filtered_df_8['Batch CC'].copy().values

X_mean_8 = df_mean_8.drop(columns=['Batch Sample Index', 'Base']).copy().values
y_mean_8 = df_mean_8.index.copy().values

X_mean_8c = df_mean_8c.drop(columns=['Batch Sample Index', 'Base']).copy().values
y_mean_8c = df_mean_8c.index.copy().values

_, _, y_train_C_8, y_test_C_8 = train_test_split(x_8, y_c_8, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_c_8)
_, _, y_train_Bi_8, y_test_Bi_8 = train_test_split(x_8, y_bi_8, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_bi_8)
_, _, y_train_B_8, y_test_B_8 = train_test_split(x_8, y_b_8, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_b_8)
X_train_8, X_test_8, y_train_Bcc_8, y_test_Bcc_8 = train_test_split(x_8, y_bcc_8, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_bcc_8)

df_mean_32 = filtered_df_32.copy().groupby(['Batch CC']).mean()
df_mean_32c = filtered_df_32.copy().groupby(['Contegees Number']).mean()


X_mean_32 = df_mean_32.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number']).copy().values
y_mean_32 = df_mean_32.index.copy().values

X_mean_32c = df_mean_32c.drop(columns=['Batch Index', 'Batch Sample Index', 'Base']).copy().values
y_mean_32c = df_mean_32c.index.copy().values


x_32 = filtered_df_32.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).copy().values
y_b_32 = filtered_df_32['Base'].copy().values
y_c_32 = filtered_df_32['Contegees Number'].copy().values
y_bi_32 = filtered_df_32['Batch Index'].copy().values
y_bcc_32 = filtered_df_32['Batch CC'].copy().values


_, _, y_train_C_32, y_test_C_32 = train_test_split(x_32, y_c_32, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_c_32)
_, _, y_train_Bi_32, y_test_Bi_32 = train_test_split(x_32, y_bi_32, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_bi_32)
_, _, y_train_B_32, y_test_B_32 = train_test_split(x_32, y_b_32, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_b_32)
X_train_32, X_test_32, y_train_Bcc_32, y_test_Bcc_32 = train_test_split(x_32, y_bcc_32, test_size = 0.3, random_state = 42, shuffle=False) #, stratify=y_bcc_32)

























from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import warnings

from sklearn import metrics
from sklearn.model_selection import GridSearchCV, LeaveOneOut
# warnings.filterwarnings("ignore")
warnings.filterwarnings(action='once')

filtered_df_8 = df_8

x_8 = filtered_df_8.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).copy().values
y_c_8= filtered_df_8['Contegees Number'].copy().values


scaler_8 = StandardScaler()
data_norm_8 = scaler_8.fit_transform(x_8)

# X_train_8, X_test_8, y_train_c_8, y_test_c_8 = train_test_split(data_norm_8, y_c_8, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_c_8)


pca_2_8 = PCA(n_components=2)
principalComponents_8 = pca_2_8.fit_transform(data_norm_8)

lda_2_8 = LDA(n_components=2)
X_LDA_8 = lda_2_8.fit_transform(data_norm_8, y_c_8)

# pca_3 = PCA(n_components=3)
# principalComponents3 = pca_3.fit_transform(X_train_32)

# wcss_pca = []
# for i in range(1, 15):
#   kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_pca.fit(principalComponents_8)
#   wcss_pca.append(kmeans_pca.inertia_)

# plt.plot(range(1, 15), wcss_pca)
# plt.title('KMEANS Clusters with PCA 2D Plot')
# plt.show()


# wcss_lda = []
# for i in range(1, 15):
#   kmeans_lda = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_lda.fit(X_LDA_8)
#   wcss_lda.append(kmeans_lda.inertia_)

# plt.plot(range(1, 15), wcss_lda)
# plt.title('KMEANS Clusters with LDA 2D Plot')
# plt.show()


kmeans_pca_8 = KMeans(n_clusters=3, init='k-means++', random_state=42)
kmeans_pca_8.fit(principalComponents_8)

kmeans_lda_8 = KMeans(n_clusters=3, init='k-means++', random_state=42)
kmeans_lda_8.fit(X_LDA_8)


# labels_kmeans_pca = kmeans_pca.predict(principalComponents)
# labels_kmeans_lda = kmeans_lda.predict(X_LDA)
# plt.subplots()
# for i, j in enumerate(np.unique(y_c_8)):
#   plt.scatter(principalComponents_8[y_c_8 == j, 0], principalComponents_8[y_c_8 == j, 1],
#                     label = j)

# plt.title('PCA 2D')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

# plt.show()

# plt.subplots()
# for i, j in enumerate(np.unique(kmeans_pca_8.labels_)):
#   plt.scatter(principalComponents_8[kmeans_pca_8.labels_ == j, 0], principalComponents_8[kmeans_pca_8.labels_ == j, 1],
#                     label = j)
# for i, j in enumerate(kmeans_pca_8.cluster_centers_):
#   plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
#   plt.annotate(i, (j[0], j[1] + 0.25))

# plt.title('PCA 2D Plot with KMEANS 3 Clusters')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

#   # show scatter plot
# plt.show()



plt.subplots()
for i, j in enumerate(np.unique(y_c_8)):
  plt.scatter(X_LDA_8[y_c_8 == j, 0], X_LDA_8[y_c_8 == j, 1],
                    label = j)

plt.title('LDA 2D')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

plt.show()

plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_8.labels_)):
  plt.scatter(X_LDA_8[kmeans_lda_8.labels_ == j, 0], X_LDA_8[kmeans_lda_8.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_8.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('LDA 2D Plot with KMEANS 3 Clusters')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

  # show scatter plot
plt.show()


# kmeans_pca_8 = KMeans(n_clusters=4, init='k-means++', random_state=42)
# kmeans_pca_8.fit(principalComponents_8)

kmeans_lda_8 = KMeans(n_clusters=4, init='k-means++', random_state=42)
kmeans_lda_8.fit(X_LDA_8)


# plt.subplots()
# for i, j in enumerate(np.unique(kmeans_pca_8.labels_)):
#   plt.scatter(principalComponents_8[kmeans_pca_8.labels_ == j, 0], principalComponents_8[kmeans_pca_8.labels_ == j, 1],
#                     label = j)
# for i, j in enumerate(kmeans_pca_8.cluster_centers_):
#   plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
#   plt.annotate(i, (j[0], j[1] + 0.25))

# plt.title('PCA 2D Plot with KMEANS 4 Clusters')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

#   # show scatter plot
# plt.show()


plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_8.labels_)):
  plt.scatter(X_LDA_8[kmeans_lda_8.labels_ == j, 0], X_LDA_8[kmeans_lda_8.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_8.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('LDA 2D Plot with KMEANS 4 Clusters')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

  # show scatter plot
plt.show()

filtered_df_32 = df_32

# filtered_df_32 = filtered_df_32.sample(frac = 1)

x_32 = filtered_df_32.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).copy().values
y_c_32 = filtered_df_32['Contegees Number'].copy().values
# y_c_32 = filtered_df_32['Base'].copy().values


scaler_32 = StandardScaler()
data_norm_32 = scaler_32.fit_transform(x_32)

# from sklearn import preprocessing
  
# # label_encoder object knows how to understand word labels.
# label_encoder = preprocessing.LabelEncoder()
  
# # Encode labels in column 'species'.
# y_c_32 = label_encoder.fit_transform(y_c_32)


pca_2_32 = PCA(n_components=2)
principalComponents_32 = pca_2_32.fit_transform(data_norm_32)

pca_3_32 = PCA(n_components=3)
principalComponents_32_3 = pca_3_32.fit_transform(data_norm_32)


lda_2_32 = LDA(n_components=2)
X_LDA_32 = lda_2_32.fit_transform(data_norm_32, y_c_32)

lda_3_32 = LDA(n_components=3)
X_LDA_32_3 = lda_3_32.fit_transform(data_norm_32, y_c_32)

wcss = []
plt.rcParams["figure.figsize"] = (25,12)

for i in range(1, 11):
  kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)
  kmeans.fit(X_LDA_32)
  wcss.append(kmeans.inertia_)
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

# # pca_3 = PCA(n_components=3)
# # principalComponents3 = pca_3.fit_transform(X_train_32)

# wcss_pca = []
# for i in range(1, 15):
#   kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_pca.fit(principalComponents_32)
#   wcss_pca.append(kmeans_pca.inertia_)

# plt.plot(range(1, 15), wcss_pca)
# plt.title('KMEANS Clusters with PCA 2D Plot')
# plt.show()


# wcss_lda = []
# for i in range(1, 15):
#   kmeans_lda = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_lda.fit(X_LDA_32)
#   wcss_lda.append(kmeans_lda.inertia_)

# plt.plot(range(1, 15), wcss_lda)
# plt.title('KMEANS Clusters with LDA 2D Plot')
# plt.show()
K = 3

kmeans_pca_32 = KMeans(n_clusters=K, init='k-means++', random_state=42)
kmeans_pca_32.fit(principalComponents_32)

kmeans_lda_32 = KMeans(n_clusters=K, init='k-means++', random_state=42)
pred_y_lda_32 = kmeans_lda_32.fit_predict(X_LDA_32)

kmeans_pca3_32 = KMeans(n_clusters=K, init='k-means++', random_state=42)
kmeans_pca3_32.fit(principalComponents_32_3)

kmeans_lda3_32 = KMeans(n_clusters=K, init='k-means++', random_state=42)
pred_y_lda3_32 = kmeans_lda3_32.fit_predict(X_LDA_32_3)

plt.rcParams["figure.figsize"] = (15,10)

plt.rcParams["figure.figsize"] = (12,8)
for i, j in enumerate(np.unique(y_c_32)):
  plt.scatter(X_LDA_32[y_c_32 == j, 0], X_LDA_32[y_c_32 == j, 1],
                    label = j)

plt.title('LDA 2D')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend
plt.show()
plt.rcParams["figure.figsize"] = (12,8)

plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_32.labels_)):
  plt.scatter(X_LDA_32[kmeans_lda_32.labels_ == j, 0], X_LDA_32[kmeans_lda_32.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_32.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=300, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('LDA 2D Plot with KMEANS Clusters')
plt.xlabel('LDA1') # for Xlabel

plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

# show scatter plot
plt.show()

# pred_y_lda_32 = label_encoder.inverse_transform(pred_y_lda_32)
y_c_32 = label_encoder.inverse_transform(y_c_32)

y_c_32

pred_y_lda_32

pred_y_lda_32 = list(map(lambda x: 21 if x == 0 else 
                         4 if x == 1 else 
                         26 if x == 2 else 
                         32 if x == 3 else 
                         10 if x == 4 else 
                         231 if x == 5 else 
                         6 if x == 6 else 
                         11 if x == 7 else
                         232, pred_y_lda_32))

unique, counts = np.unique(pred_y_lda_32, return_counts=True)
dict(zip(unique, counts))

unique, counts = np.unique(pred_y_lda_32, return_counts=True)
dict(zip(unique, counts))

# Import label encoder
y_c_32

unique, counts = np.unique(y_c_32, return_counts=True)
dict(zip(unique, counts))

unique, counts = np.unique(y_c_32, return_counts=True)
dict(zip(unique, counts))

# print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(y_c_32, pred_y_lda_32))
# print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X_LDA_32, pred_y_lda_32))
print(metrics.classification_report(y_c_32, pred_y_lda_32))

pred_y_lda_32

elbow_index = 9

param_grid = {'init': ['k-means++'],
'n_init': [10],
'max_iter': [10, 20, 30],
'n_clusters':[elbow_index-1,elbow_index,elbow_index+1] }

kmeans = KMeans()

clf = GridSearchCV(kmeans, param_grid, cv=LeaveOneOut(), verbose=1)
clf.fit(X_LDA_32)

#Print the best parameters 
print("Best parameters: ", clf.best_params_)
pred_y = clf.predict(X_LDA_32)

plt.scatter(X[pred_y == 0, 0], X[pred_y == 0, 1])
plt.scatter(X[pred_y == 1, 0], X[pred_y == 1, 1])

plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')
plt.show()

print("Adjusted Rand Index: %0.3f" % metrics.adjusted_rand_score(y_c_32, pred_y_lda_32))
print("Silhouette Coefficient: %0.3f" % metrics.silhouette_score(X, pred_y_lda_32))
print(metrics.classification_report(y_c_32, pred_y_lda_32))

plt.rcParams["figure.figsize"] = (25,12)

# labels_kmeans_pca = kmeans_pca.predict(principalComponents)
# labels_kmeans_lda = kmeans_lda.predict(X_LDA)
plt.subplots()
for i, j in enumerate(np.unique(y_c_32)):
  plt.scatter(principalComponents_32[y_c_32 == j, 0], principalComponents_32[y_c_32 == j, 1],
                    label = j)

plt.title('PCA 2D')
plt.xlabel('PCA1') # for Xlabel
plt.ylabel('PCA2') # for Ylabel
plt.legend() # to show legend
plt.show()

fig = plt.figure()
plt.rcParams["figure.figsize"] = (25,12)
ax = fig.add_subplot(projection='3d')
for i, j in enumerate(np.unique(y_c_32)):
  ax.scatter(principalComponents_32_3[y_c_32 == j, 0], principalComponents_32_3[y_c_32 == j, 1], principalComponents_32_3[y_c_32 == j, 2],
                  label = j)

plt.title('PCA 3D')
ax.set_xlabel('PCA1')
ax.set_ylabel('PCA2')
ax.set_zlabel('PCA3')
plt.legend()
plt.show()

plt.rcParams["figure.figsize"] = (25,12)
for i, j in enumerate(np.unique(kmeans_pca_32.labels_)):
  plt.scatter(principalComponents_32[kmeans_pca_32.labels_ == j, 0], principalComponents_32[kmeans_pca_32.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_pca_32.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=300, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('PCA 2D Plot with KMEANS Clusters')
plt.xlabel('PCA1') # for Xlabel
plt.ylabel('PCA2') # for Ylabel
plt.legend() # to show legend
# show scatter plot
plt.show()

fig = plt.figure()
plt.rcParams["figure.figsize"] = (25,12)
ax = fig.add_subplot(projection='3d')
for i, j in enumerate(np.unique(kmeans_pca3_32.labels_)):
  ax.scatter(principalComponents_32_3[kmeans_pca3_32.labels_ == j, 0], principalComponents_32_3[kmeans_pca3_32.labels_ == j, 1], principalComponents_32_3[kmeans_pca3_32.labels_ == j, 2],
                    label = j)
for i, j in enumerate(kmeans_pca3_32.cluster_centers_):
  # print(i, j)
  ax.scatter(j[0], j[1], j[2], label = i, marker='+', s=300, c='k')
  # ax.annotate(i, (j[0], j[1]))

plt.title('PCA 3D Plot with KMEANS Clusters')
ax.set_xlabel('PCA1')
ax.set_ylabel('PCA2')
ax.set_zlabel('PCA3')
plt.legend()
plt.show()

plt.rcParams["figure.figsize"] = (12,8)
for i, j in enumerate(np.unique(y_c_32)):
  plt.scatter(X_LDA_32[y_c_32 == j, 0], X_LDA_32[y_c_32 == j, 1],
                    label = j)

plt.title('LDA 2D')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend
plt.show()

plt.rcParams["figure.figsize"] = (25,12)

plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_32.labels_)):
  plt.scatter(X_LDA_32[kmeans_lda_32.labels_ == j, 0], X_LDA_32[kmeans_lda_32.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_32.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=300, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('LDA 2D Plot with KMEANS Clusters')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

# show scatter plot
plt.show()

kmeans_lda_32.labels_

kmeans_lda3_32.cluster_centers_

fig = plt.figure()
plt.rcParams["figure.figsize"] = (25,12)
ax = fig.add_subplot(projection='3d')
for i, j in enumerate(np.unique(kmeans_lda3_32.labels_)):
  ax.scatter(X_LDA_32_3[kmeans_lda3_32.labels_ == j, 0], X_LDA_32_3[kmeans_lda3_32.labels_ == j, 1], X_LDA_32_3[kmeans_lda3_32.labels_ == j, 2],
                    label = j, alpha=0.1, linewidths=0.5)
  
for i, j in enumerate(kmeans_lda3_32.cluster_centers_):
  ax.scatter(j[0], j[1], j[2]+3, label = kmeans_lda3_32.labels_[i], marker='+', s=500, c='k', alpha=1, linewidths=3)
  # ax.annotate(i, (j[0], j[1]))

plt.title('LDA 3D Plot with KMEANS Clusters')
ax.set_xlabel('PCA1')
ax.set_ylabel('PCA2')
ax.set_zlabel('PCA3')
plt.legend()
plt.show()











def plot_3d(component1,component2,component3, y):
  fig = go.Figure(data=[go.Scatter3d(
          x=component1,
          y=component2,
          z=component3,
          mode='markers',
          marker=dict(
              size=10,
              color=y,                # set color to an array/list of desired values
              # colorscale='Rainbow',   # choose a colorscale
              opacity=1,
              line_width=1
          )
      )])
  # tight layout
  fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=1000,height=1000)
  # fig.layout.template = 'plotly_dark'
      
  fig.show()



_32_0 = filtered_df_32[filtered_df_32['Base'] == 0].copy()
x_32_0 = _32_0.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).values
y_c_32_0 = _32_0['Contegees Number'].copy().values


scaler_32_0 = StandardScaler()
data_norm_32_0 = scaler_32_0.fit_transform(x_32_0)

# X_train_32, X_test_32, y_train_c_32, y_test_c_32 = train_test_split(data_norm_32, y_c_32, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_c_32)


pca_2_32_0 = PCA(n_components=2)
principalComponents_32_0 = pca_2_32_0.fit_transform(data_norm_32_0)

lda_2_32_0 = LDA(n_components=2)
X_LDA_32_0 = lda_2_32_0.fit_transform(data_norm_32_0, y_c_32_0)


# pca_3 = PCA(n_components=3)
# principalComponents3 = pca_3.fit_transform(X_train_32)

# wcss_pca = []
# for i in range(1, 15):
#   kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_pca.fit(principalComponents_32_0)
#   wcss_pca.append(kmeans_pca.inertia_)

# plt.plot(range(1, 15), wcss_pca)
# plt.title('KMEANS Clusters with PCA 2D Plot')
# plt.show()


# wcss_lda = []
# for i in range(1, 15):
#   kmeans_lda = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_lda.fit(X_LDA_32_0)
#   wcss_lda.append(kmeans_lda.inertia_)

# plt.plot(range(1, 15), wcss_lda)
# plt.title('KMEANS Clusters with LDA 2D Plot')
# plt.show()

K = 3
# kmeans_pca_32_0 = KMeans(n_clusters=K, init='k-means++', random_state=42)
# kmeans_pca_32_0.fit(principalComponents_32_0)

kmeans_lda_32_0 = KMeans(n_clusters=K, init='k-means++', random_state=42)
kmeans_lda_32_0.fit(X_LDA_32_0)
plt.rcParams["figure.figsize"] = (12,8)


# labels_kmeans_pca = kmeans_pca.predict(principalComponents)
# labels_kmeans_lda = kmeans_lda.predict(X_LDA)
# plt.subplots()
# for i, j in enumerate(np.unique(y_c_32_0)):
#   plt.scatter(principalComponents_32_0[y_c_32_0 == j, 0], principalComponents_32_0[y_c_32_0 == j, 1],
#                     label = j)

# plt.title('PCA 2D')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

# plt.show()

# plt.subplots()
# for i, j in enumerate(np.unique(kmeans_pca_32_0.labels_)):
#   plt.scatter(principalComponents_32_0[kmeans_pca_32_0.labels_ == j, 0], principalComponents_32_0[kmeans_pca_32_0.labels_ == j, 1],
#                     label = j)
# for i, j in enumerate(kmeans_pca_32_0.cluster_centers_):
#   plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
#   plt.annotate(i, (j[0], j[1] + 0.25))

# plt.title('PCA 2D Plot with KMEANS Clusters')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

# # show scatter plot
# plt.show()



plt.subplots()
for i, j in enumerate(np.unique(y_c_32_0)):
  plt.scatter(X_LDA_32_0[y_c_32_0 == j, 0], X_LDA_32_0[y_c_32_0 == j, 1],
                    label = j)

plt.title('LDA 2D')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

plt.show()

plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_32_0.labels_)):
  plt.scatter(X_LDA_32_0[kmeans_lda_32_0.labels_ == j, 0], X_LDA_32_0[kmeans_lda_32_0.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_32_0.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))
plt.title('LDA 2D Plot with KMEANS Clusters')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

# show scatter plot
plt.show()

_32_1 = filtered_df_32[filtered_df_32['Base'] == 1].copy()
x_32_1 = _32_1.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).values
y_c_32_1 = _32_1['Contegees Number'].copy().values


scaler_32_1 = StandardScaler()
data_norm_32_1 = scaler_32_1.fit_transform(x_32_1)

# X_train_32, X_test_32, y_train_c_32, y_test_c_32 = train_test_split(data_norm_32, y_c_32, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_c_32)


pca_2_32_1 = PCA(n_components=2)
principalComponents_32_1 = pca_2_32_1.fit_transform(data_norm_32_1)

lda_2_32_1 = LDA(n_components=2)
X_LDA_32_1 = lda_2_32_1.fit_transform(data_norm_32_1, y_c_32_1)


# pca_3 = PCA(n_components=3)
# principalComponents3 = pca_3.fit_transform(X_train_32)

# wcss_pca = []
# for i in range(1, 15):
#   kmeans_pca = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_pca.fit(principalComponents_32_1)
#   wcss_pca.append(kmeans_pca.inertia_)

# plt.plot(range(1, 15), wcss_pca)
# plt.title('KMEANS Clusters with PCA 2D Plot')
# plt.show()


# wcss_lda = []
# for i in range(1, 15):
#   kmeans_lda = KMeans(n_clusters=i, init='k-means++', random_state=42)
#   kmeans_lda.fit(X_LDA_32_1)
#   wcss_lda.append(kmeans_lda.inertia_)

# plt.plot(range(1, 15), wcss_lda)
# plt.title('KMEANS Clusters with LDA 2D Plot')
# plt.show()

K = 3
# kmeans_pca_32_1 = KMeans(n_clusters=K, init='k-means++', random_state=42)
# kmeans_pca_32_1.fit(principalComponents_32_1)

kmeans_lda_32_1 = KMeans(n_clusters=K, init='k-means++', random_state=42)
kmeans_lda_32_1.fit(X_LDA_32_1)


# labels_kmeans_pca = kmeans_pca.predict(principalComponents)
# labels_kmeans_lda = kmeans_lda.predict(X_LDA)
# plt.subplots()
# for i, j in enumerate(np.unique(y_c_32_1)):
#   plt.scatter(principalComponents_32_1[y_c_32_1 == j, 0], principalComponents_32_1[y_c_32_1 == j, 1],
#                     label = j)

# plt.title('PCA 2D')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

# plt.show()

# plt.subplots()
# for i, j in enumerate(np.unique(kmeans_pca_32_1.labels_)):
#   plt.scatter(principalComponents_32_1[kmeans_pca_32_1.labels_ == j, 0], principalComponents_32_1[kmeans_pca_32_1.labels_ == j, 1],
#                     label = j)
# for i, j in enumerate(kmeans_lda_32_1.cluster_centers_):
#   plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
#   plt.annotate(i, (j[0], j[1] + 0.25))

# plt.title('PCA 2D Plot with KMEANS Clusters')
# plt.xlabel('PCA1') # for Xlabel
# plt.ylabel('PCA2') # for Ylabel
# plt.legend() # to show legend

#   # show scatter plot
# plt.show()
plt.rcParams["figure.figsize"] = (12,8)



plt.subplots()
for i, j in enumerate(np.unique(y_c_32_1)):
  plt.scatter(X_LDA_32_1[y_c_32_1 == j, 0], X_LDA_32_1[y_c_32_1 == j, 1],
                    label = j)

plt.title('LDA 2D')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

plt.show()

plt.subplots()
for i, j in enumerate(np.unique(kmeans_lda_32_1.labels_)):
  plt.scatter(X_LDA_32_1[kmeans_lda_32_1.labels_ == j, 0], X_LDA_32_1[kmeans_lda_32_1.labels_ == j, 1],
                    label = j)
for i, j in enumerate(kmeans_lda_32_1.cluster_centers_):
  plt.scatter(j[0], j[1], label = i, marker='+', s=220, c='k')
  plt.annotate(i, (j[0], j[1] + 0.25))

plt.title('LDA 2D Plot with KMEANS Clusters')
plt.xlabel('LDA1') # for Xlabel
plt.ylabel('LDA2') # for Ylabel
plt.legend() # to show legend

  # show scatter plot
plt.show()





plot_2d(X_LDA, y_train_C_32)

def plot_2d(components, y):
  # fig = go.Figure(data=go.Scatter(
  #     x = components[:, 0],
  #     y = components[:, 1],
  #     mode='markers',
  #     marker=dict(
  #         size=20,
  #         color=y, #set color equal to a variable
  #         # colorscale='bone', # one of plotly colorscales
  #         showscale=True,
  #         line_width=1
  #     )
  #     ))
  # fig.update_layout(margin=dict( l=100,r=100,b=100,t=100),width=1400,height=800)                 
  # fig.layout.template = 'plotly_dark'
      
  # fig.show()

  for i, j in enumerate(np.unique(y)):
    plt.scatter(components[y == j, 0], components[y == j, 1],
                      label = j)

    plt.title('2D')
    plt.xlabel('PCA1') # for Xlabel
    plt.ylabel('PCA2') # for Ylabel
    plt.legend() # to show legend

    # show scatter plot
  plt.show()

scaler = StandardScaler()
data_norm = scaler.fit_transform(X_train_32)
pca = PCA(n_components=2)
principalComponents = pca.fit_transform(data_norm)

kmeans = KMeans(n_clusters=len(np.unique(y_train_C_32)))
kmeans.fit(principalComponents)
lda = LDA(n_components=2)
X_LDA = lda.fit_transform(data_norm, y_train_C_32)
# print('Duration: {} seconds'.format(time.time() - start))
principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2']) # ,'principal component 3'])

test_data_norm = scaler.transform(X_test_32)
# Predict clusters on test data
test_data_pca = pca.transform(test_data_norm)
test_data_lda = lda.transform(test_data_norm)
# test_labels_knn = knn.predict(test_data_norm)
test_labels_kmeans = kmeans.predict(test_data_lda)

unique, counts = np.unique(test_labels_kmeans, return_counts=True)
dict(zip(unique, counts))

def plot_2d(components, y, ):

  for i, j in enumerate(np.unique(y)):
    plt.scatter(components[y == j, 0], components[y == j, 1],
                      label = j)

    plt.title('LDA 2D')
    plt.xlabel('PCA1') # for Xlabel
    plt.ylabel('PCA2') # for Ylabel
    plt.legend() # to show legend

    # show scatter plot
  plt.show()

  plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_kmeans)
  plt.title("LDA 2D Plot with KMEANS Clusters")
  
  # for i, j in enumerate(np.unique(test_labels_kmeans)):
  #   plt.scatter(test_data_pca[y == j, 0], test_data_pca[y == j, 1],
  #                     label = j)

  #   plt.title('PCA 2D Plot with KMEANS Clusters')
  #   plt.xlabel('PCA1') # for Xlabel
  #   plt.ylabel('PCA2') # for Ylabel
  #   plt.legend() # to show legend

  #   # show scatter plot
  plt.show()

  # # 2D plot of the test data using LDA with KMEANS clusters
  # plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_kmeans)
  # plt.title("LDA 2D Plot with KMEANS Clusters")
  # plt.show

plot_2d(principalComponents, y_c_8)

plot_2d(X_LDA, y_c_32)

plot_2d(principalComponents, y_c_32)













scaler = StandardScaler()
data_norm = scaler.fit_transform(x_32)
pca = PCA(n_components=3)
principalComponents = pca.fit_transform(data_norm)
X_LDA = LDA(n_components=3).fit_transform(data_norm, y_c_32)

# print('Duration: {} seconds'.format(time.time() - start))
principal = pd.DataFrame(data = principalComponents
             , columns = ['principal component 1', 'principal component 2','principal component 3'])

def plot_3d(component1,component2,component3, y):
  fig = go.Figure(data=[go.Scatter3d(
          x=component1,
          y=component2,
          z=component3,
          mode='markers',
          marker=dict(
              size=10,
              color=y,                # set color to an array/list of desired values
              colorscale='Rainbow',   # choose a colorscale
              opacity=1,
              line_width=1
          )
      )])
  # tight layout
  fig.update_layout(margin=dict(l=50,r=50,b=50,t=50),width=1800,height=1000)
  # fig.layout.template = 'plotly_dark'
      
  fig.show()

plot_3d(X_LDA[:, 0], X_LDA[:, 1], X_LDA[:, 2], y_c_32)

plot_3d(principalComponents[:, 0],principalComponents[:, 1],principalComponents[:, 2], y_c_32)



plt.rcParams["figure.autolayout"] = True
warnings.filterwarnings("ignore")

def visualize_results(test_data, pca, lda, knn, kmeans, train_data_pca, train_data_lda, train_labels):
    """
    Function to visualize the results of PCA, LDA, KNN and KMEAN models
    """
    # Predict clusters on test data
    test_data_pca = pca.transform(test_data)
    test_data_lda = lda.transform(test_data)
    test_labels_knn = knn.predict(test_data_pca)
    test_labels_kmeans = kmeans.predict(test_data_pca)

    # # 3D plot of the training data using PCA
    # fig = plt.figure()
    # ax = fig.add_subplot(111, projection='3d')
    # ax.scatter(train_data_pca[:, 0], train_data_pca[:, 1], train_data_pca[:, 2], c=train_labels)
    # plt.title("PCA 3D Plot")
    # plt.show()
    
    # # 3D plot of the training data using LDA
    # fig = plt.figure()
    # ax = fig.add_subplot(111, projection='3d')
    # ax.scatter(train_data_lda[:, 0], train_data_lda[:, 1], train_data_lda[:, 2], c=train_labels)
    # plt.title("LDA 3D Plot")
    # plt.show()

    # 2D plot of the training data using PCA
    # plt.scatter(train_data_pca[:, 0], train_data_pca[:, 1], c=train_labels)
    # plt.title("PCA 2D Plot")
    # plt.show()
    
    
    unique_labels = list(set(train_labels))
    colors = plt.cm.rainbow(np.linspace(0, 1, len(unique_labels)))
    label_colors = {label: color for label, color in zip(unique_labels, colors)}
    for label in unique_labels:
      plt.scatter(train_data_pca[:, 0], train_data_pca[:, 1], c=label_colors[label], label=label)
      plt.xlabel("First Principal Component")
      plt.ylabel("Second Principal Component")
      plt.title("PCA 2D Plot")
      plt.legend(title="Classes")
    plt.show()
    
    # # 2D plot of the training data using LDA
    # plt.scatter(train_data_lda[:, 0], train_data_lda[:, 1], c=train_labels)
    # plt.title("LDA 2D Plot")
    # plt.show()

    # # 2D plot of the test data using PCA with KNN clusters
    # plt.scatter(test_data_pca[:, 0], test_data_pca[:, 1], c=test_labels_knn)
    # plt.title("PCA 2D Plot with KNN Clusters")
    # plt.show()
    
    # # 2D plot of the test data using LDA with KNN clusters
    # plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_knn)
    # plt.title("LDA 2D Plot with KNN Clusters")
    # plt.show()

    # # 2D plot of the test data using PCA with KMEANS clusters
    # plt.scatter(test_data_pca[:, 0], test_data_pca[:, 1], c=test_labels_kmeans)
    # plt.title("PCA 2D Plot with KMEANS Clusters")
    # plt.show()
    
    # # 2D plot of the test data using LDA with KMEANS clusters
    # plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_kmeans)
    # plt.title("LDA 2D Plot with KMEANS Clusters")
    # plt.show

# Training the models and visualizing the results
pca, lda, knn, kmeans, train_data_pca, train_data_lda = train_models_v(X_train_32, y_train_C_32, X_test_32) # change the parameters as needed

visualize_results(X_test_32, pca, lda, knn, kmeans, train_data_pca, train_data_lda, y_test_C_32)







print('X Shape: ', X_train_32.shape)
print('y_b Shape: ',y_train_Bcc_32.shape)
# print('y_c Shape: ',y_c.shape)
# print('y_bi Shape: ',y_bi.shape)

from sklearn.cluster import KMeans
# KMEAN clustering
kmeans = KMeans(n_clusters=9)
kmeans.fit(train_data_pca)

test_labels_kmeans = kmeans.predict(test_data_pca)

def plot_explain_variance_vs_components(X):
    
    def get_variance(X, n):
        scaler = StandardScaler()
        pca = PCA(n_components=n)

        pca.fit(scaler.fit_transform(X))

        return pca.explained_variance_ratio_.cumsum()[-1:]

    def transform_pca(X, n):

        pca = PCA(n_components=n)
        pca.fit(X)
        X_new = pca.inverse_transform(pca.transform(X))

        return X_new
    
    scaler = StandardScaler()
    data_rescaled = scaler.fit_transform(X)

    var_list = []
    for i in range(1, X.shape[1]+1):
        var_list.append(get_variance(X, i)[0])
    # #     print(f'Components Num: {i}\t', get_variance(X, i)[0], 
    # #           '\tCumulative Variance')


    plt.rcParams["figure.figsize"] = (15,10)
    fig1, ax = plt.subplots()

    pca = PCA().fit(data_rescaled)

    xi = np.arange(1, X.shape[1]+1, step=1)
    y = np.cumsum(pca.explained_variance_ratio_)

    plt.ylim(0.0,1.1)
    plt.plot(xi, y, marker='o', linestyle='-', color='black')

    plt.xlabel('Number of Components')
    plt.xticks(np.arange(1, X.shape[1]+1, step=1)) 
    plt.ylabel('Cumulative variance (%)')
    plt.title('The number of components needed to explain variance')

    plt.axhline(y=0.99, color='grey', linestyle='--')
    plt.text(11.5, 1.015, '99% cut-off threshold', color = 'black', fontsize=16)

    plt.axhline(y=0.95, color='grey', linestyle='--')
    plt.text(7.3, 0.92, '95% cut-off threshold', color = 'black', fontsize=16)

    ax.grid()
    plt.tight_layout()
    # plt.savefig('pcavisualize_1.png', dpi=300)

    
    rows = 7
    cols = 3
    comps = 1

    fig, axes = plt.subplots(rows, 
                             cols,  
                             sharex=True, 
                             sharey=True)

    for row in range(rows):
        for col in range(cols):
            try:
                X_new = transform_pca(data_rescaled, comps)
                ax = sns.scatterplot(x=data_rescaled[:, 0], 
                                     y=data_rescaled[:, 1], 
                                     ax=axes[row, col], 
                                     color='grey', 
                                     alpha=.3)
                ax = sns.scatterplot(x=X_new[:, 0], 
                                     y=X_new[:, 1], 
                                     ax=axes[row, col], 
                                     color='black')
                ax.set_title(f'PCA Components: {comps} ({round(var_list[comps-1], 3)})');

                comps += 1
            except:
                pass
    plt.tight_layout()
    # plt.savefig('pcavisualize_2.png', dpi=300)
    plt.show()


def plot_pca(X, X_mean, y_mean, Mean, y_test, X_test, y_train, y_, x_):
    sc = StandardScaler()

    if Mean == False:

        X_train_sc = sc.fit_transform(X)
        X_test_sc = sc.transform(X_test)

        pca = PCA(n_components = 2)
        lda = LDA(n_components=2)
        
        
        X_train_pca = pca.fit_transform(X_train_sc)
        X_test_pca = pca.transform(X_test_sc)
        
        
        X_train_lda = lda.fit_transform(X_train_sc, y_train)
#         X_test_lda = lda.transform(X_test_sc)

#         classifier_b = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
#         classifier_b.fit(X_train_pca, y_train_B)

        X_set = X_train_pca
        # from sklearn.cluster import KMeans
        # # KMEAN clustering
        # kmeans = KMeans(n_clusters=5)
        # kmeans.fit(X_test_pca)


        # PCA1, PCA2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 1,
        #              stop = X_set[:, 0].max() + 1, step = 0.01),
        #              np.arange(start = X_set[:, 1].min() - 1,
        #              stop = X_set[:, 1].max() + 1, step = 0.01))
        # plt.rcParams["figure.figsize"] = (20, 10)
        
        # plt.contourf(PCA1, PCA2, kmeans.predict(np.array([PCA1.ravel(),
        #                                                         PCA2.ravel()]).T).reshape(PCA1.shape), alpha = 0.75, cmap = 'bone')
        # plt.xlim(PCA1.min(), PCA1.max())
        # plt.ylim(PCA2.min(), PCA2.max())
        # plt.rcParams["figure.figsize"] = (20, 10)

        for i, j in enumerate(np.unique(y_train)):
            plt.scatter(X_set[y_train == j, 0], X_set[y_train == j, 1],
                        label = j)

        plt.title('kmeans (Training set)')
        plt.xlabel('PCA1') # for Xlabel
        plt.ylabel('PCA2') # for Ylabel
        plt.legend() # to show legend

        # show scatter plot
        plt.show()
#         colors = np.random.randint(0, 9, size=9) # colors for 9 points

        fig = go.Figure(data=go.Scatter(x = X_train_lda[:, 0], y = X_train_lda[:, 1],
                                        mode='markers', marker=dict(
                                            size=20,
                                            color=y_train, #set color equal to a variable
                                            # colorscale='Rainbow', # one of plotly colorscales
                                            showscale=True,
                                            line_width=1
                                        )))
        fig.update_layout(margin=dict( l=100,r=100,b=100,t=100),width=1000,height=1000)                 
        # fig.layout.template = 'plotly_dark'

        fig.show()

    else:
        X_sc = sc.fit_transform(X_mean)
        
        X_sc_ = sc.fit_transform(x_)
        
#         X_train_sc = sc.fit_transform(X)
#         pca_ = PCA(n_components = 2)
#         X_train_pca = pca_.fit_transform(X_train_sc)

#         lda = LDA(n_components=2)
        
        pca = PCA(n_components = 2)
        
        X_pca_mean = pca.fit_transform(X_sc)
        X_pca_mean_ = pca.fit_transform(X_sc_)
        print(X_sc.shape)
        print(y_mean.shape)
        print('\n')
#         X_lda_mean = lda.fit_transform(X_sc, y_mean)
#         X_lda_mean_ = lda.fit_transform(X_sc_, y_)


#         classifier_b = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)
#         classifier_b.fit(X_train_pca, y_train_B)

#         PCA1, PCA2 = np.meshgrid(np.arange(start = X_pca_mean[:, 0].min() - 1,
#                              stop = X_pca_mean[:, 0].max() + 1, step = 0.01),
#                              np.arange(start = X_set[:, 1].min() - 1,
#                              stop = X_pca_mean[:, 1].max() + 1, step = 0.01))

#         plt.contourf(PCA1, PCA2, alpha = 0.75,cmap = 'bone')
# #         plt.contourf(PCA1, PCA2, classifier_b.predict(np.array([PCA1.ravel(),
# #                      PCA2.ravel()]).T).reshape(PCA1.shape), alpha = 0.75,
# #                      cmap = 'bone')

#         plt.xlim(PCA1.min(), PCA1.max())
#         plt.ylim(PCA2.min(), PCA2.max())
        #   ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd']
        plt.rcParams["figure.figsize"] = (20, 10)
    
        for i, j in enumerate(np.unique(y_train)):
            if j[-1] == str(1):
                plt.scatter(X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1],
                            label = j, c='#1f77b4', marker='^', s=150)
                plt.annotate(j, (X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1]))
            if j[-1] == str(2):
                plt.scatter(X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1],
                            label = j, c= '#ff7f0e', marker='^', s=150)
                plt.annotate(j, (X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1]))

            if j[-1] == str(3):
                plt.scatter(X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1],
                            label = j, c='#2ca02c', marker='^', s=150)
                plt.annotate(j, (X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1]))
            if j[-1] == str(4):
                plt.scatter(X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1],
                            label = j, c='#d62728', marker='^', s=150)
                plt.annotate(j, (X_pca_mean[y_train == j, 0], X_pca_mean[y_train == j, 1]))
            if j[-1] == str(5):
                plt.scatter(X_pca_mean[y_set == j, 0], X_pca_mean[y_set == j, 1],
                            label = j, c='#9467bd', marker='^', s=150)
                plt.annotate(j, (X_pca_mean[y_train == j, 0], X_pca_mean[y_set == j, 1]))
                
        for i, j in enumerate(np.unique(y_)):
            plt.scatter(X_pca_mean_[y_ == str(j), 0], X_pca_mean_[y_ == str(j), 1],
                            label = j, c='black', marker='+', s=300)
            plt.annotate(j, (X_pca_mean_[y_ == str(j), 0], X_pca_mean_[y_ == str(j), 1]))

        plt.title('RandomForestClassifier (Testing set)')
        plt.xlabel('PCA1') # for Xlabel
        plt.ylabel('PCA2') # for Ylabel
        plt.legend() # to show legend

        # show scatter plot
        plt.show()
        
#         fig = go.Figure(data=go.Scatter(x = X_lda_mean[:, 0], y = X_lda_mean[:, 1],
#                                         mode='markers', marker=dict(
#                                             size=20,
#                                             color=X_lda_mean[:, 1], #set color equal to a variable
#                                             colorscale='Rainbow', # one of plotly colorscales
#                                             showscale=True,
#                                             line_width=1
#                                         )))
#         fig.update_layout(margin=dict( l=100,r=100,b=100,t=100),width=2000,height=1200)                 
#         fig.layout.template = 'plotly_dark'

#         fig.show()

plot_explain_variance_vs_components(x_32)



plot_explain_variance_vs_components(x_8)

unique, counts = np.unique(y_c_32, return_counts=True)
dict(zip(unique, counts))

plot_pca(X=x_32, X_mean=X_mean_32, y_mean=y_mean_32, Mean=False, y_test=y_test_C_32,
         X_test=X_test_32, y_train=y_c_32, y_=np.array(list(map(str, y_mean_32c))), x_=X_mean_32c)

y_train_Bcc_32

















plot_pca(X=X_train_32, X_mean=X_mean_32, y_set=y_mean_32, Mean=True, X_test=X_test_32, 
         y_train_B=y_train_B_32, y_=np.array(list(map(str, y_mean_32c))), x_=X_mean_32c)

plot_pca(X=X_train_8, X_mean=X_mean_8, y_set=y_mean_8, Mean=True, X_test=X_test_8, y_train_B=y_train_B_8, y_=np.array(list(map(str, y_mean_8c))), x_=X_mean_8c)

df__32 = filtered_df_32.iloc[:, :-5]
features32 = filtered_df_32.columns[:-5].tolist()

fig = px.scatter_matrix(
    df__32,
    
    dimensions=features32,
    color=filtered_df_32["Batch CC"]
)
fig.update_traces(diagonal_visible=False)
fig.show()

X32 = filtered_df_32[features32]

pca = PCA(n_components=2)
components = pca.fit_transform(X32)

fig = px.scatter(components, x=0, y=1, color=filtered_df_32['Base'])
fig.show()

n_components = 2

pca = PCA(n_components=n_components)
components = pca.fit_transform(df__8)

total_var = pca.explained_variance_ratio_.sum() * 100

labels = {str(i): f"PC {i+1} ({var:.1f}%)" for i, var in enumerate(pca.explained_variance_ratio_ * 100)}
labels['color'] = 'Batch CC'

plt.rcParams["figure.figsize"] = (15,10)
fig = px.scatter_matrix(
    components,
    color=filtered_df_8["Batch CC"],
    dimensions=range(n_components),
    labels=labels,
    title=f'Total Explained Variance: {total_var:.2f}%',
)
fig.update_traces(diagonal_visible=False)
fig.show()

labels = {str(i): f"PC {i+1} ({var:.1f}%)" for i, var in enumerate(pca.explained_variance_ratio_ * 100)}
labels['color'] = 'Base'

plt.rcParams["figure.figsize"] = (15,10)
fig1 = px.scatter_matrix(
    components,
    color=filtered_df_8["Base"],
    dimensions=range(n_components),
    labels=labels,
    title=f'Total Explained Variance: {total_var:.2f}%',
)
fig1.update_traces(diagonal_visible=False)
fig1.show()

n_components = 2

pca = PCA(n_components=n_components)
components = pca.fit_transform(df__32)

total_var = pca.explained_variance_ratio_.sum() * 100

labels = {str(i): f"PC {i+1} ({var:.1f}%)" for i, var in enumerate(pca.explained_variance_ratio_ * 100)}
labels['color'] = 'Batch CC'

plt.rcParams["figure.figsize"] = (15,10)
fig = px.scatter_matrix(
    components,
    color=filtered_df_32["Batch CC"],
    dimensions=range(n_components),
    labels=labels,
    title=f'Total Explained Variance: {total_var:.2f}%',
)
fig.update_traces(diagonal_visible=False)
fig.show()

labels = {str(i): f"PC {i+1} ({var:.1f}%)" for i, var in enumerate(pca.explained_variance_ratio_ * 100)}
labels['color'] = 'Base'

plt.rcParams["figure.figsize"] = (15,10)
fig1 = px.scatter_matrix(
    components,
    color=filtered_df_32["Base"],
    dimensions=range(n_components),
    labels=labels,
    title=f'Total Explained Variance: {total_var:.2f}%',
)
fig1.update_traces(diagonal_visible=False)
fig1.show()

pca = PCA(n_components=3)
components = pca.fit_transform(X8)

total_var = pca.explained_variance_ratio_.sum() * 100

fig = px.scatter_3d(
    components, x=0, y=1, z=2, color=filtered_df_8['Base'],
    title=f'Total Explained Variance: {total_var:.2f}%',
    labels={'0': 'PC 1', '1': 'PC 2', '2': 'PC 3'}
)
fig.show()

pca = PCA()
pca.fit(X32)
exp_var_cumul = np.cumsum(pca.explained_variance_ratio_)

px.area(
    x=range(1, exp_var_cumul.shape[0] + 1),
    y=exp_var_cumul,
    labels={"x": "# Components", "y": "Explained Variance"}
)



df_mean_8 = filtered_df_8.copy().groupby(['Batch CC']).mean()
df_mean_8c = filtered_df_8.copy().groupby(['Contegees Number']).mean()

x_8 = filtered_df_8.drop(columns=['Batch Index', 'Batch Sample Index', 'Base', 'Contegees Number', 'Batch CC']).copy().values
y_b_8 = filtered_df_8['Base'].copy().values
y_c_8 = filtered_df_8['Contegees Number'].copy().values
y_bi_8 = filtered_df_8['Batch Index'].copy().values
y_bcc_8 = filtered_df_8['Batch CC'].copy().values

X_mean_8 = df_mean_8.drop(columns=['Batch Sample Index', 'Base']).copy().values
y_mean_8 = df_mean_8.index.copy().values

X_mean_8c = df_mean_8c.drop(columns=['Batch Sample Index', 'Base']).copy().values
y_mean_8c = df_mean_8c.index.copy().values

_, _, y_train_C_8, y_test_C_8 = train_test_split(x_8, y_c_8, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_c_8)
_, _, y_train_Bi_8, y_test_Bi_8 = train_test_split(x_8, y_bi_8, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_bi_8)
_, _, y_train_B_8, y_test_B_8 = train_test_split(x_8, y_b_8, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_b_8)
X_train_8, X_test_8, y_train_Bcc_8, y_test_Bcc_8 = train_test_split(x_8, y_bcc_8, test_size = 0.3, random_state = 42, shuffle=True, stratify=y_bcc_8)

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
from sklearn.neighbors import KNeighborsClassifier



lda = LDA(n_components=n_components)
train_data_lda = lda.fit_transform(X_train_8, y_train_C_8)

def train_models(train_data, train_labels, n_components=2, n_neighbors=9, n_clusters=9):
    """
    Function to train PCA, LDA, KNN and KMEAN models on the train data
    """
    # Perform PCA on train data
    pca = PCA(n_components=n_components)
    train_data_pca = pca.fit_transform(train_data)
    
#     # Perform LDA on train data
#     lda = LDA(n_components=n_components)
#     train_data_lda = lda.fit_transform(train_data, train_labels)
    
#     # KNN clustering
#     knn = KNeighborsClassifier(n_neighbors=n_neighbors)
#     knn.fit(train_data_pca, train_labels)
    
    # KMEAN clustering
    kmeans = KMeans(n_clusters=n_clusters)
    kmeans.fit(train_data_pca)

    return pca, kmeans #, knn, lda

def visualize_results(train_data, train_labels, test_data, pca, kmeans): #, knn, lda):
    """
    Function to visualize the results of PCA, LDA, KNN and KMEAN models
    """
    # Predict clusters on test data
    test_data_pca = pca.transform(test_data)
#     test_data_lda = lda.transform(test_data)
#     test_labels_knn = knn.predict(test_data_pca)
    test_labels_kmeans = kmeans.predict(test_data_pca)
    train_data_pca = pca.fit_transform(train_data)
    

#     # 3D plot of the training data using PCA
#     fig = plt.figure()
#     ax = fig.add_subplot(111, projection='3d')
#     ax.scatter(train_data_pca[:, 0], train_data_pca[:, 1], train_data_pca[:, 2], c=train_labels)
#     plt.title("PCA 3D Plot")
#     plt.show()
    
#     # 3D plot of the training data using LDA
#     fig = plt.figure()
#     ax = fig.add_subplot(111, projection='3d')
#     ax.scatter(train_data_lda[:, 0], train_data_lda[:, 1], train_data_lda[:, 2], c=train_labels)
#     plt.title("LDA 3D Plot")
#     plt.show()
    for i, j in enumerate(np.unique(train_labels)):
        plt.scatter(train_data_pca[train_labels == j, 0], train_data_pca[train_labels == j, 1],
                            label = j)
        plt.title("PCA 2D Plot")
        plt.xlabel('PCA1') # for Xlabel
        plt.ylabel('PCA2') # for Ylabel
        plt.legend() # to show legend
    plt.show()

    # 2D plot of the training data using PCA
#     plt.scatter(train_data_pca[:, 0], train_data_pca[:, 1], c=train_labels)
#     plt.title("PCA 2D Plot")
#     plt.show()
    
#     # 2D plot of the training data using LDA
#     plt.scatter(train_data_lda[:, 0], train_data_lda[:, 1], c=train_labels)
#     plt.title("LDA 2D Plot")
#     plt.show()

#     # 2D plot of the test data using PCA with KNN clusters
#     plt.scatter(test_data_pca[:, 0], test_data_pca[:, 1], c=test_labels_knn)
#     plt.title("PCA 2D Plot with KNN Clusters")
#     plt.show()
    
#     # 2D plot of the test data using LDA with KNN clusters
#     plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_knn)
#     plt.title("LDA 2D Plot with KNN Clusters")
#     plt.show()

    # 2D plot of the test data using PCA with KMEANS clusters
    plt.scatter(test_data_pca[:, 0], test_data_pca[:, 1], c=test_labels_kmeans)
    plt.title("PCA 2D Plot with KMEANS Clusters")
    plt.xlabel('PCA1') # for Xlabel
    plt.ylabel('PCA2') # for Ylabel
    plt.legend() # to show leg
    plt.show()
    
#     # 2D plot of the test data using LDA with KMEANS clusters
#     plt.scatter(test_data_lda[:, 0], test_data_lda[:, 1], c=test_labels_kmeans)
#     plt.title("LDA 2D Plot with KMEANS Clusters")
#     plt.show

unique, counts = np.unique(y_train_C_8, return_counts=True)
vc = dict(zip(unique, counts))
vc



# Training the models and visualizing the results
pca, kmeans = train_models(X_train_32, y_train_C_32) 

visualize_results(X_train_32, y_train_C_32, X_test_32, pca, kmeans) # lda, knn,



# Training the models and visualizing the results
pca, kmeans = train_models(X_train_8, y_train_C_8)

# Training the models and visualizing the results
# pca, lda, knn, kmeans = train_models(train_data, train_labels) 

visualize_results(X_train_8, y_train_C_8, X_test_8, pca, kmeans) # lda, knn,

from sklearn.metrics import silhouette_score

def check_components(train_data, train_labels, max_components=5):
    """
    Function to check the optimal number of components for clustering techniques
    """
    scores = []
    for i in range(2, max_components+1):
        # Perform PCA on train data
        pca = PCA(n_components=i)
        train_data_pca = pca.fit_transform(train_data)
        
        # KMEANS clustering
        kmeans = KMeans(n_clusters=len(np.unique(train_labels)))
        kmeans.fit(train_data_pca)
        score = silhouette_score(train_data_pca, kmeans.labels_)
        scores.append((i,score))
    return scores







def clean_db(X):
    # drop 0.01 variance columns:
    del_col = []
    for i in X.columns[:-1]:
        if np.std(X.loc[:, i]) < 0.01 * np.mean(X.loc[:, i]):
            del_col.append(i)
        print(del_col, 'have 1% variance, will be dropped from db')
        X = X.drop(del_col, axis=1)

        # drop 99% correlated columns
        corr_db = pd.DataFrame(np.corrcoef(X.transpose().astype(float)),
                               index=X.columns,
                               columns=X.columns)
        del_col = []
        for c_index, c in enumerate(corr_db.columns):
            for b in corr_db.index[c_index + 1:]:
                if corr_db.loc[c, b] >= 0.99 and b != c:
                    if b not in del_col:
                        del_col.append(b)
        print("deleting column ", del_col)
        print("Total deleted columns = ", len(del_col))
        X = X.drop(del_col, axis=1)

        return X
    
def stat_tests(data, a=0.05):
        pass_t_test = []
        d1 = data.copy()
        f_n = d1.drop(['Batch Index=Repetition number ', 'Batch Sample Index=Sampling N.', 
                       'Base= Molecule A/Molecule B ', 'Contamination N.'], axis=1).columns.values.tolist()

        def two_side_mannwhitneyu(x, y, col_name, alfa):

            u, p_val = mannwhitneyu(x, y, alternative='two-sided')

            if p_val < alfa:
                pass_t_test.append(col_name)

        for i in range(len(f_n)):
            two_side_mannwhitneyu(data.loc[data['Base= Molecule A/Molecule B '] == 0, data.columns == f_n[i]],
                                  data.loc[data['Base= Molecule A/Molecule B '] == 1, data.columns == f_n[i]],  f_n[i], a/len(f_n))

        print("num remain:", len(pass_t_test))
        print("num start:", len(f_n))
        print("num pop:", len(f_n) - len(pass_t_test))
        pass_t_test.append('Batch Index=Repetition number ')
        pass_t_test.append('Batch Sample Index=Sampling N.')
        pass_t_test.append('Base= Molecule A/Molecule B ')
        pass_t_test.append('Contamination N.')
        
#         data = data[pass_t_test]
        data = clean_db(data)

        return data